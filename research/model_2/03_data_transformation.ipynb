{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\DataScience\\\\Projects\\\\Predictive_maintenance\\\\Predictive_Maintenance_JetEngine\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\DataScience\\\\Projects\\\\Predictive_maintenance\\\\Predictive_Maintenance_JetEngine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    ground_truth_data_path: Path\n",
    "    columns: list\n",
    "    columns_to_drop: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.PredictiveMaintenance.constants import *\n",
    "from src.PredictiveMaintenance.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        cols_schema=self.schema.COLUMNS\n",
    "        cols_to_drop_schema=self.schema.COLUMNS_TO_DROP\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            ground_truth_data_path=config.ground_truth_data_path,\n",
    "            columns=list(cols_schema.keys()),\n",
    "            columns_to_drop=list(cols_to_drop_schema.keys())\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.PredictiveMaintenance import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        logger.info(\"data transformation starteD\")\n",
    "        self.config = config \n",
    "    \n",
    "    def Calculate_RUL(self,df):\n",
    "        max_cycles = df.groupby('Engine Number')['Times/ in cycle'].max()\n",
    "        merged = df.merge(max_cycles.to_frame(name='max_time_cycle'), left_on='Engine Number',right_index=True)\n",
    "        merged[\"RUL\"] = merged[\"max_time_cycle\"] - merged['Times/ in cycle']\n",
    "        merged[\"RUL\"].head(3)\n",
    "        merged = merged.drop(\"max_time_cycle\", axis=1)\n",
    "        return merged\n",
    "    \n",
    "    def impute_outliers(self,data):\n",
    "        sensors=data.drop(columns=['Engine Number', 'Times/ in cycle'],axis=1)\n",
    "        sensors=data.columns\n",
    "        for col in sensors:\n",
    "            q1 = data[col].quantile(0.25)\n",
    "            q3 = data[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - (1.5 * iqr)\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "\n",
    "            data.loc[data[col] < lower_bound, col] = data[col].median()\n",
    "            data.loc[data[col] > upper_bound, col] = data[col].median()\n",
    "        return data\n",
    "    \n",
    "    def train_test_spliting(self):\n",
    "\n",
    "        train_data=pd.read_csv(self.config.train_data_path,names=self.config.columns,sep=\"\\s+\",header=None)\n",
    "        test_data=pd.read_csv(self.config.test_data_path,names=self.config.columns,sep=\"\\s+\",header=None)\n",
    "        ground_truth_data=pd.read_csv(self.config.ground_truth_data_path,names=[\"RUL\"])\n",
    "        logger.info(\"Data loaDeD\")\n",
    "\n",
    "        print(train_data.shape)\n",
    "        train_data=self.Calculate_RUL(train_data)\n",
    "        logger.info(train_data[\"RUL\"].head(2))\n",
    "\n",
    "        logger.info(\"rul calcualateD\")\n",
    "        print(train_data.shape)\n",
    "\n",
    "        X_train=train_data.iloc[:,:-1]\n",
    "        y_train=train_data.iloc[:,-1]\n",
    "        logger.info(y_train.head(2))\n",
    "\n",
    "        X_train=self.impute_outliers(X_train)\n",
    "        logger.info(\"outliers imputeD\")\n",
    "\n",
    "        X_train=X_train.drop(self.config.columns_to_drop,axis=1)\n",
    "        \n",
    "        print(X_train.shape)\n",
    "\n",
    "        print(test_data.shape)#.drop(columns=[\"Engine Number\",\"Times/ in cycle\",\"Burner fuel-air ratio\", \"Required fan speed\",\"Total Fan inlet temperature\",\"Total Fan inlet pressure\", \"Required fan conversion speed\", \"Mach Number(Setting_2)\", \"Bleed enthalpy\",\"Engine pressure ratio(P50/P2)\",\"TRA(Setting_3)\"],axis=1)\n",
    "        test_data=test_data.groupby(\"Engine Number\").last().reset_index().drop(columns=[\"Engine Number\",\"Times/ in cycle\",\"Burner fuel-air ratio\", \"Required fan speed\",\"Total Fan inlet temperature\",\"Total Fan inlet pressure\", \"Required fan conversion speed\", \"Mach Number(Setting_2)\", \"Bleed enthalpy\",\"Engine pressure ratio(P50/P2)\",\"TRA(Setting_3)\"],axis=1)\n",
    "        \n",
    "        columns = list(X_train.columns[1:])\n",
    "\n",
    "        logger.info(\"Data preprocessing start\")\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"Numeric\", StandardScaler(), columns)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        preprocessor_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cols_names = list(X_train.columns[:])\n",
    "\n",
    "        X_train = pd.DataFrame(preprocessor_pipeline.fit_transform(X_train), columns=cols_names)\n",
    "        X_test = pd.DataFrame(preprocessor_pipeline.transform(test_data), columns=cols_names)\n",
    "\n",
    "        X_train.to_csv(os.path.join(self.config.root_dir, \"X_train.csv\"),index = False)\n",
    "        y_train.to_csv(os.path.join(self.config.root_dir,\"y_train.csv\"),index=False)\n",
    "        X_test.to_csv(os.path.join(self.config.root_dir, \"X_test.csv\"),index = False)\n",
    "        ground_truth_data.to_csv(os.path.join(self.config.root_dir,\"RUL.csv\"),index=False)\n",
    "\n",
    "        logger.info(\"Splited data into training and test sets\")\n",
    "        logger.info(X_train.shape)\n",
    "        logger.info(X_test.shape)\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-09 15:01:38,925: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-03-09 15:01:38,935: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-09 15:01:38,943: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-09 15:01:38,949: INFO: common: created directory at: artifacts]\n",
      "[2024-03-09 15:01:38,952: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-03-09 15:01:38,952: INFO: 2736021010: data transformation starteD]\n",
      "[2024-03-09 15:01:39,782: INFO: 2736021010: Data loaDeD]\n",
      "(53759, 26)\n",
      "[2024-03-09 15:01:39,813: INFO: 2736021010: 0    148\n",
      "1    147\n",
      "Name: RUL, dtype: int64]\n",
      "[2024-03-09 15:01:39,819: INFO: 2736021010: rul calcualateD]\n",
      "(53759, 27)\n",
      "[2024-03-09 15:01:39,822: INFO: 2736021010: 0    148\n",
      "1    147\n",
      "Name: RUL, dtype: int64]\n",
      "[2024-03-09 15:01:39,953: INFO: 2736021010: outliers imputeD]\n",
      "(53759, 15)\n",
      "(33991, 26)\n",
      "c  (259, 15)\n",
      "[2024-03-09 15:01:39,963: INFO: 2736021010: Data preprocessing start]\n",
      "[2024-03-09 15:01:41,014: INFO: 2736021010: Splited data into training and test sets]\n",
      "[2024-03-09 15:01:41,019: INFO: 2736021010: (53759, 15)]\n",
      "[2024-03-09 15:01:41,019: INFO: 2736021010: (259, 15)]\n",
      "(53759, 15)\n",
      "(259, 15)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.train_test_spliting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envrul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
